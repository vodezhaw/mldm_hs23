{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAJRKdv9QA4C",
        "tags": []
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn import svm\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "\n",
        "%config InlineBackend.figure_formats = ['svg']\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)\n",
        "\n",
        "cmap= mpl.colors.ListedColormap(['red', 'black', 'blue'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfTwSIFT9UyK"
      },
      "source": [
        "In order to plot the decision boundary and margins we will used the `DecisionBoundaryDisplay.from_estimator` from `sklearn` and wrap it into a function for reuse:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "-Ej03R1G9UyM"
      },
      "outputs": [],
      "source": [
        "def plot_svc_decision_function(model, X, y, ax):\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap)\n",
        "\n",
        "    DecisionBoundaryDisplay.from_estimator(\n",
        "        estimator=model, X=X,ax=ax,\n",
        "        response_method=\"decision_function\",\n",
        "        plot_method=\"contour\",\n",
        "        levels=[-1, 0, 1],\n",
        "        colors=[\"grey\", \"black\", \"grey\"],\n",
        "        linestyles=[\"--\", \"-\", \"--\"],\n",
        "    )\n",
        "\n",
        "    # indicate the support vectors\n",
        "    ax.scatter(X[model.support_][:, 0], X[model.support_][:, 1],\n",
        "               facecolors='none', edgecolors='black', s=100)\n",
        "\n",
        "    ax.set_xlabel(r'$x_1$', fontsize=16)\n",
        "    ax.set_ylabel(r'$x_2$', fontsize=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTVOKVltsG4d"
      },
      "source": [
        "# TASK 1. SVM (2 Points): Soft Margin Classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA0LWVdnQzlU"
      },
      "source": [
        "Let's work on the same artificial data with two classes from part 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbvdP62EIWQF",
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "X, y = datasets.make_blobs(n_samples=50, centers=2,\n",
        "                  random_state=0, cluster_std=0.60)\n",
        "\n",
        "_, ax = plt.subplots(figsize=(5, 5))\n",
        "ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=cmap)\n",
        "ax.set_xlabel(r'$x_1$', fontsize=16)\n",
        "ax.set_ylabel(r'$x_2$', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Lkm6FC-1KsD",
        "jupyter": {
          "outputs_hidden": false
        },
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "model = svm.SVC(kernel='linear', C=0.1) # initialize the model\n",
        "model.fit(X, y) # fit the model = learn the decision boundaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Z-ZXrmEi9UyO"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots(figsize=(5, 5))\n",
        "\n",
        "plot_svc_decision_function(model, X, y, ax)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd8_ENRCi5ch"
      },
      "source": [
        "In the plot you see a solid and 2 dashed lines.\n",
        "\n",
        "The solid line shows the decision boundary, meaning points on one side will be assigned to the blue class and on the other side to the red class. In this case all\n",
        "data points lie on the correct side of the decision boundary, meaning our classifier has successfully learned to separate the classes.\n",
        "\n",
        "The dashed lines visualize the margin of the SVM classifier. You can see that a\n",
        "few points lie inside the margin. This is because, in practice, we use a soft-margin implementation for SVM. This means that we allow data points to be inside the margin, or even on the wrong side of the boundary, but during training we \"punish\" these points.\n",
        "\n",
        "In the next excercise we will investigate the effect of the `C` parameter of. This\n",
        "parameter controls how much we \"punish\" points inside (or on the wrong side) of the margin."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0JJcOdK1KsE"
      },
      "source": [
        "### Regularization\n",
        "For each value `C` given in the code fragment below create an instance of SVC and set its parameter C accordingly. Train the model on the data `X` from the cells above. Plot the decision boundary using `plot_svc_decision_function(model, X, y, ax)`. Observe the effect from the plots and discuss with your colleagues whether it is in line with your expectation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sn2zmKrr1KsE"
      },
      "outputs": [],
      "source": [
        "for c in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
        "    # create an instance of SVC and set its parameter C\n",
        "    # call its .fit method to train it on X and y\n",
        "\n",
        "    model = ...\n",
        "\n",
        "    _, ax = plt.subplots(figsize=(5, 5))\n",
        "    plot_svc_decision_function(model, X, y, ax)\n",
        "    ax.set_title(f\"C={c}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-l9Ebxgp1KsE"
      },
      "source": [
        "### We add a new datapoint\n",
        "\n",
        "We will now add a new red data point, that lies very close to the cluster of blue points.\n",
        "\n",
        "In real world datasets, we run into this situation all the time. One reason can be that some data points really do look very similar but belong to different classes (e.g. imagine distinguishing a crocodile from an alligator). Another reason could be that the data point would really belong to the other (blue) class but has been annotated badly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otatVaHS1KsF",
        "tags": []
      },
      "outputs": [],
      "source": [
        "X2 = np.append(X, [[1., 1.5]], axis=0)\n",
        "y2 = np.append(y, [0], axis=0)\n",
        "\n",
        "_, ax = plt.subplots(figsize=(5, 5))\n",
        "ax.scatter(X2[:, 0], X2[:, 1], c=y2, s=50, cmap=cmap)\n",
        "ax.set_xlabel(r'$x_1$', fontsize=16)\n",
        "ax.set_ylabel(r'$x_2$', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Awtugu1h1KsF"
      },
      "source": [
        "Rerun the examle from above, i.e. train a linear `SVC` with different values of `C` and see what changes.\n",
        "\n",
        "Pay particular attention to how the new data point changes the outcome."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9Nod5tv1KsF"
      },
      "outputs": [],
      "source": [
        "for c in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
        "    # create an instance of LinearSVC and set its parameter C\n",
        "    # call its .fit method to train it on X2 and y2\n",
        "\n",
        "    model = ...\n",
        "\n",
        "    _, ax = plt.subplots(figsize=(5, 5))\n",
        "    plot_svc_decision_function(model, ...)\n",
        "    ax.set_title(f\"C={c}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON8ogrmw1q4k"
      },
      "source": [
        "## ðŸ“¢ **HAND-IN** ðŸ“¢: Submit the following task in Moodle:\n",
        "\n",
        "Based on the experiments in **task 1** above, which value for `C` would you choose for the new data, and **why**?  \n",
        "And Upload the image of the corresponding plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz4yA62HAyB6"
      },
      "source": [
        "# TASK 2. Support Vector Machines - Unbalanced Data (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4BAX9lf1KsG"
      },
      "source": [
        "How to handle unbalanced classes with SVM? Below you see an example of an unbalanced problem. There are 1000 red samples and just 100 blue samples. The SVM is very sensitive to this. Here we will explore the SVMs behaviour in the unbalanced case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuh0m5zy1KsG",
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# we create clusters with 1000 and 100 points\n",
        "rng = np.random.RandomState(0)\n",
        "n_samples_1 = 1000\n",
        "n_samples_2 = 100\n",
        "X_unbal = np.r_[1.5 * rng.randn(n_samples_1, 2), 0.5 * rng.randn(n_samples_2, 2) + [3, 1]]\n",
        "y_unbal = [0] * (n_samples_1) + [1] * (n_samples_2)\n",
        "\n",
        "_, ax  =plt.subplots(figsize=(5.0, 5.0))\n",
        "ax.grid()\n",
        "\n",
        "ax.scatter(X_unbal[:, 0], X_unbal[:, 1], c=y_unbal, cmap=cmap,\n",
        "            edgecolors='k')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV76eoPA1KsG"
      },
      "source": [
        "### Default solution\n",
        "\n",
        "In the cells below, you see a simple way to classify the above data.\n",
        "* We use linear `SVC` with default parameters as our model and fit it on the unbalanced data\n",
        "* We visualize the resulting decision boundary\n",
        "* We compute performance metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcSVZZPopzFP",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# This should look familiar by now.\n",
        "\n",
        "clf = svm.SVC(kernel='linear', C=1.0)\n",
        "clf.fit(X_unbal, y_unbal)\n",
        "\n",
        "_, ax = plt.subplots(figsize=(5, 5))\n",
        "plot_svc_decision_function(clf, X_unbal, y_unbal, ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QZiBkt-qHLF",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# get predictions from the classifier\n",
        "# it will assign red to all points to the left of the solid line and blue to the points on the right side\n",
        "y_pred = clf.predict(X_unbal)\n",
        "\n",
        "# the function classification_report computes precision, recall, and F1 score\n",
        "# and returns a nicely formatted string\n",
        "# it takes the true labels as argument y_true and predicted labels as y_pred\n",
        "# the digits argument controls how many decimal points are printed\n",
        "# the support column in the output shows how many data points belong to each class\n",
        "print(classification_report(y_true=y_unbal, y_pred=y_pred, digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmJlEUOTwx0G"
      },
      "source": [
        "### Handle unbalanced data using class weights\n",
        "\n",
        "In this exercise we will explore how the `class_weight` parameter can be used to handle unbalanced data.\n",
        "\n",
        "We have seen previously that `C` \"punishes\" data points that lie on the wrong side of the classification boundary. The `class_weight` parameter follows a similar idea. It allows us to control how harshly we punish points on the wrong\n",
        "side for each class separately. For example, we could choose to force more blue points to be on the right of the boundary, by giving them more *weight*. This means they will be counted more during training and mis-classifications are punished more.\n",
        "\n",
        "The default value for the `class_weight` parameter is `None` and means both\n",
        "classes will have a weight of 1.\n",
        "\n",
        "You can manually set weights using a dictionary:\n",
        "```\n",
        "my_class_weights = {\n",
        "  0: 1.0,\n",
        "  1: 1.0,\n",
        "}\n",
        "```\n",
        "The keys correspond to the class labels (in our case 0 for red and 1 for blue)\n",
        "and the values correspond to the weight we want to set. We can then set `class_weight=my_class_weights`.\n",
        "\n",
        "\n",
        "Finally, you can specify `class_weight='balanced'`. In that case, `sklearn` tries to automatically determine good class weights. Let $n$ be the total number of samples (1100 in our case), $c$ the total number of classes (2 in our case), and $n_i$ the number of samples in class $i$ (in our case $n_0 = 1000$ and $n_1 = 100$, then it computes the class weights $w_i$ as $w_i = \\frac{n}{c n_i}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0YF9vUtJA02"
      },
      "source": [
        "Choose a proper parameter for `class_weight` for example by manually trying out different values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3IhX4Xh1KsI",
        "jupyter": {
          "outputs_hidden": false
        },
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# fit the model using weighted classes\n",
        "\n",
        "# change the class weights\n",
        "my_class_weights = {\n",
        "    0: 1.0,  # weight for red class\n",
        "    1: 1.0,  # weight for blue class\n",
        "}\n",
        "wclf = ...  # create a linear SVC and set its class_weight parameter\n",
        "wclf.fit(X_unbal, y_unbal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfNefU3d1KsI"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots(figsize=(5, 5))\n",
        "plot_svc_decision_function(wclf, X_unbal, y_unbal, ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5ur7dOkxdaM"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_true=y_unbal, y_pred=wclf.predict(X_unbal), digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3w3Hop_46Af"
      },
      "source": [
        "## ðŸ“¢ **HAND-IN** ðŸ“¢: Post following results in Moodle:\n",
        "\n",
        "1. The code how you called the linear `SVC` with proper `class_weight` settings. From the **task 2**.\n",
        "2. The classification report from the **task 2**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBJKSAuw79su"
      },
      "source": [
        "# Task 3. SVM Kernel Trick (6 points)\n",
        "In this task we will investigate the case of non-linearly separable data. In order to handle such a case, the **Kernel Trick** can be used. We transform our data and map it into a **higher dimensional feature space** (e.g., if the data had two features (2D-space), it becomes 3D-space). The goal is that after the transformation to the higher dimensional space, the classes will be linearly separable. The decision boundary can then be fitted to separate the classes and make predictions. The decision boundary will be a hyperplane in this higher dimensional space.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1be1nzl1KsJ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "X_non_linear, y_non_linear = datasets.make_circles(100, factor=.1, noise=.1)\n",
        "\n",
        "_, ax = plt.subplots(figsize=(5, 5))\n",
        "ax.scatter(X_non_linear[:, 0], X_non_linear[:, 1], c=y_non_linear, cmap=cmap)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBDHhAPb1KsJ"
      },
      "source": [
        "### Default model\n",
        "\n",
        "Below you can see what happens if we naively train a linear `SVC` model on this non-linear dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XI_lTRAu1KsJ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "clf = svm.SVC(kernel='linear').fit(X_non_linear, y_non_linear)\n",
        "\n",
        "_, ax = plt.subplots(figsize=(5, 5))\n",
        "plot_svc_decision_function(clf, X_non_linear, y_non_linear, ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAYJ2Qxf8CFi",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_true=y_non_linear, y_pred=clf.predict(X_non_linear), digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3RtWVcn1KsJ"
      },
      "source": [
        "### Manually Adding a Helper Dimension\n",
        "\n",
        "It is clear that no linear discrimination will ever be able to separate this data. We can think about how we might project the data into a higher dimension such that a linear separator would be sufficient. In the code below we compute a new value `r` based on the data points. Adding `r` as a new dimension to our data, we will see that the data becomes linearly separable.\n",
        "\n",
        "We compute $r = e^{-||x||^2}$. We chose this because the data points lie on circles and $||x||^{2}$ corresponds to the radius of the circle that a data point lies on. The exponential is often used for numerical stability. However in this case, the picture would look the same when plotting $-||x||^{2}$ directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJ_cZCwx1KsJ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "r = np.exp(-(X_non_linear ** 2).sum(1))\n",
        "\n",
        "fig1=plt.figure(figsize=(10, 10))\n",
        "ax = fig1.add_subplot(projection='3d')\n",
        "ax.scatter3D(X_non_linear[:, 0], X_non_linear[:, 1], r,\n",
        "             c=y_non_linear, s=50, cmap=cmap)\n",
        "ax.view_init(elev=15, azim=60)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_zlabel('r')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XR6byXE9UyU"
      },
      "source": [
        "### 3A Linear Classification in the extended feature space\n",
        "\n",
        "Task: Append the additional feature `r` to the original data `X_non_linear` and train a linear `SVC` in the extended feature space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XK1zorHy9UyU"
      },
      "outputs": [],
      "source": [
        "X_3d = ...\n",
        "clf_3d = svm.SVC(kernel='linear').fit(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "9la9uIBQ9UyV"
      },
      "source": [
        "Have the accuracy metrics improved?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "r-srCiuB9UyV"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_true=y_non_linear, y_pred=clf_3d.predict(X_3d), digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvkUZ3ZS9UyV"
      },
      "source": [
        "We can now visualise the learned decision boundary in the extended feature space together with the transformed data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "yJVS_YHD9UyV"
      },
      "outputs": [],
      "source": [
        "# get the parameters from the optimised model that specify the decision boundary\n",
        "w = clf_3d.coef_[0]\n",
        "\n",
        "b=clf_3d.intercept_[0]\n",
        "\n",
        "fig1=plt.figure(figsize=(10, 10))\n",
        "ax = fig1.add_subplot(projection='3d')\n",
        "ax.scatter3D(X_non_linear[:, 0], X_non_linear[:, 1], r,\n",
        "             c=y_non_linear, s=50, cmap=cmap)\n",
        "ax.view_init(elev=15, azim=60)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_zlabel('r')\n",
        "\n",
        "# Surface plot of the decision boundary (a plane) from the\n",
        "# optimised parameters\n",
        "xx, yy = np.meshgrid(np.linspace(-1,1,2),np.linspace(-1,1,2))\n",
        "# 0=b+w_0*x+w_1*y+w_2*z  ->  rearrange for z\n",
        "z = -(b + w[0]*xx * w[1]*yy) / w[2]\n",
        "ax.plot_surface(xx, yy, z, alpha=0.5)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COmUozBL9UyW"
      },
      "source": [
        "## ðŸ“¢ **HAND-IN** ðŸ“¢: Submit the following task in Moodle:\n",
        "\n",
        "Report for **task 3A** the accuracy achieved with the linear SVM in the extended feature space and upload the image of the 3D-plot including the decision boundary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V54dBD161KsK"
      },
      "source": [
        "### 3B The Kernel Trick\n",
        "**Note** that in this case it was relatively straight forward to find a suitable new dimension `r` which allows to separate the data linearly. However, this is not feasible for real world datasets. We will now use the RBF kernel on the unchanged data to let the SVM find a proper projection by itself.\n",
        "To use the kernel trick, we need to set a non-linear `kernel`.\n",
        "\n",
        "Use `kernel='rbf'` and fit an `SVC` model on `X_non_linear` and `y_non_linear`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VfmCiV71KsK"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# create an SVC an set its kernel parameter to 'rbf'\n",
        "# train / fit it on X_non_linear and y_non_linear\n",
        "clf = ...\n",
        "\n",
        "_, ax = plt.subplots(figsize=(5, 5))\n",
        "plot_svc_decision_function(clf, X_non_linear, y_non_linear, ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCcdm8I38XKr"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_true=y_non_linear, y_pred=clf.predict(X_non_linear), digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JkHrkdl97W-"
      },
      "source": [
        "## ðŸ“¢ **HAND-IN** ðŸ“¢: Submit the following task in Moodle:\n",
        "\n",
        "Report for **task 3B** the accuracy achieved with the SVM with rbf-kernel"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}